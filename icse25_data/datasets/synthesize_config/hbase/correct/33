<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<property>
  <name>hbase.ipc.server.callqueue.read.ratio</name>
  <value>1</value>
    <description>Split the call queues into read and write queues.
      The specified interval (which should be between 0.0 and 1.0)
      will be multiplied by the number of call queues.
      A value of 0 indicate to not split the call queues, meaning that both read and write
      requests will be pushed to the same set of queues.
      A value lower than 0.5 means that there will be less read queues than write queues.
      A value of 0.5 means there will be the same number of read and write queues.
      A value greater than 0.5 means that there will be more read queues than write queues.
      A value of 1.0 means that all the queues except one are used to dispatch read requests.

      Example: Given the total number of call queues being 10
      a read.ratio of 0 means that: the 10 queues will contain both read/write requests.
      a read.ratio of 0.3 means that: 3 queues will contain only read requests
      and 7 queues will contain only write requests.
      a read.ratio of 0.5 means that: 5 queues will contain only read requests
      and 5 queues will contain only write requests.
      a read.ratio of 0.8 means that: 8 queues will contain only read requests
      and 2 queues will contain only write requests.
      a read.ratio of 1 means that: 9 queues will contain only read requests
      and 1 queues will contain only write requests.
    </description>
</property>

<property>
  <name>hbase.zookeeper.leaderport</name>
  <value>7776</value>
    <description>Port used by ZooKeeper for leader election.
    See https://zookeeper.apache.org/doc/r3.3.3/zookeeperStarted.html#sc_RunningReplicatedZooKeeper
    for more information.</description>
</property>

<property>
  <name>hbase.client.scanner.caching</name>
  <value>2147483647</value>
    <description>Number of rows that we try to fetch when calling next
    on a scanner if it is not served from (local, client) memory. This configuration
    works together with hbase.client.scanner.max.result.size to try and use the
    network efficiently. The default value is Integer.MAX_VALUE by default so that
    the network will fill the chunk size defined by hbase.client.scanner.max.result.size
    rather than be limited by a particular number of rows since the size of rows varies
    table to table. If you know ahead of time that you will not require more than a certain
    number of rows from a scan, this configuration should be set to that row limit via
    Scan#setCaching. Higher caching values will enable faster scanners but will eat up more
    memory and some calls of next may take longer and longer times when the cache is empty.
    Do not set this value such that the time between invocations is greater than the scanner
    timeout; i.e. hbase.client.scanner.timeout.period</description>
</property>

<property>
  <name>hbase.hstore.compaction.throughput.lower.bound</name>
  <value>26214400</value>
    <description>The target lower bound on aggregate compaction throughput, in bytes/sec. Allows
    you to tune the minimum available compaction throughput when the
    PressureAwareCompactionThroughputController throughput controller is active. (It is active by
    default.)</description>
</property>

<property>
  <name>hbase.rest.threads.max</name>
  <value>200</value>
    <description>The maximum number of threads of the REST server thread pool.
        Threads in the pool are reused to process REST requests. This
        controls the maximum number of requests processed concurrently.
        It may help to control the memory used by the REST server to
        avoid OOM issues. If the thread pool is full, incoming requests
        will be queued up and wait for some free threads.</description>
</property>

<property>
  <name>hbase.server.compactchecker.interval.multiplier</name>
  <value>2000</value>
    <description>The number that determines how often we scan to see if compaction is necessary.
        Normally, compactions are done after some events (such as memstore flush), but if
        region didn't receive a lot of writes for some time, or due to different compaction
        policies, it may be necessary to check it periodically. The interval between checks is
        hbase.server.compactchecker.interval.multiplier multiplied by
        hbase.server.thread.wakefrequency.</description>
</property>

<property>
  <name>hbase.status.published</name>
  <value>false</value>
    <description>
      This setting activates the publication by the master of the status of the region server.
      When a region server dies and its recovery starts, the master will push this information
      to the client application, to let them cut the connection immediately instead of waiting
      for a timeout.
    </description>
</property>

<property>
  <name>hbase.mob.file.cache.size</name>
  <value>1000</value>
    <description>
      Number of opened file handlers to cache.
      A larger value will benefit reads by providing more file handlers per mob
      file cache and would reduce frequent file opening and closing.
      However, if this is set too high, this could lead to a "too many opened file handlers"
      The default value is 1000.
    </description>
</property>

</configuration>
