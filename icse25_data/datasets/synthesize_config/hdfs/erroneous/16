<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<property>
  <name>dfs.datanode.address</name>
  <value>0.0.0.0:3001</value>
  <description>
    The datanode server address and port for data transfer.
  </description>
</property>

<property>
  <name>dfs.permissions.superusergroup</name>
  <value>xdgroup</value>
  <description>The name of the group of super-users.
    The value should be a single group name.
  </description>
</property>

<property>
  <name>dfs.namenode.max.objects</name>
  <value>-1</value>
  <description>The maximum number of files, directories and blocks
  dfs supports. A value of zero indicates no limit to the number
  of objects that dfs supports.
  </description>
</property>

<property>
  <name>dfs.namenode.checkpoint.dir</name>
  <value>file:::</value>
  <description>Determines where on the local filesystem the DFS secondary
      name node should store the temporary images to merge.
      If this is a comma-delimited list of directories then the image is
      replicated in all of the directories for redundancy.
  </description>
</property>

<property>
  <name>dfs.datanode.drop.cache.behind.writes</name>
  <value>false</value>
  <description>
        In some workloads, the data written to HDFS is known to be significantly
        large enough that it is unlikely to be useful to cache it in the
        operating system buffer cache. In this case, the DataNode may be
        configured to automatically purge all data from the buffer cache
        after it is written to disk.

        This may improve performance for some workloads by freeing buffer
        cache space usage for more cacheable data.

        If the Hadoop native libraries are not available, this configuration
        has no effect.
  </description>
</property>

<property>
  <name>dfs.datanode.hostname</name>
  <value>127.0.0.1</value>
  <description>
    Optional.  The hostname for the Datanode containing this
    configuration file.  Will be different for each machine.
    Defaults to current hostname.
  </description>
</property>

<property>
  <name>dfs.mover.keytab.enabled</name>
  <value>true</value>
  <description>
    Set to true to enable login using a keytab for Kerberized Hadoop.
  </description>
</property>

<property>
  <name>dfs.quota.by.storage.type.enabled</name>
  <value>true</value>
  <description>
    If true, enables quotas based on storage type.
  </description>
</property>

</configuration>
