<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<property>
  <name>dfs.datanode.httpserver.filter.handlers</name>
  <value>org.apache.hadoop.hdfs.server.datanode.web.RestCsrfPreventionFilterHandler</value>
  <description>Comma separated list of Netty servlet-style filter handlers to inject into the Datanode WebHDFS I/O path
  </description>
</property>

<property>
  <name>dfs.heartbeat.interval</name>
  <value>6s</value>
  <description>
    Determines datanode heartbeat interval in seconds.
    Can use the following suffix (case insensitive):
    ms(millis), s(sec), m(min), h(hour), d(day)
    to specify the time (such as 2s, 2m, 1h, etc.).
    Or provide complete number in seconds (such as 30 for 30 seconds).
    If no time unit is specified then seconds is assumed.
  </description>
</property>

<property>
  <name>dfs.namenode.max.extra.edits.segments.retained</name>
  <value>20000</value>
  <description>The maximum number of extra edit log segments which should be retained
  beyond what is minimally necessary for a NN restart. When used in conjunction with
  dfs.namenode.num.extra.edits.retained, this configuration property serves to cap
  the number of extra edits files to a reasonable value.
  </description>
</property>

<property>
  <name>dfs.client.datanode-restart.timeout</name>
  <value>60s</value>
  <description>
    Expert only. The time to wait, in seconds, from reception of an
    datanode shutdown notification for quick restart, until declaring
    the datanode dead and invoking the normal recovery mechanisms.
    The notification is sent by a datanode when it is being shutdown
    using the shutdownDatanode admin command with the upgrade option.
    Support multiple time unit suffix(case insensitive), as described
    in dfs.heartbeat.interval.If no time unit is specified then seconds
    is assumed.
  </description>
</property>

<property>
  <name>dfs.ha.log-roll.period</name>
  <value>240s</value>
  <description>
    How often, in seconds, the StandbyNode should ask the active to
    roll edit logs. Since the StandbyNode only reads from finalized
    log segments, the StandbyNode will only be as up-to-date as how
    often the logs are rolled. Note that failover triggers a log roll
    so the StandbyNode will be up to date before it becomes active.
    Support multiple time unit suffix(case insensitive), as described
    in dfs.heartbeat.interval.If no time unit is specified then seconds
    is assumed.
  </description>
</property>

<property>
  <name>dfs.ha.tail-edits.period</name>
  <value>60s</value>
  <description>
    How often, the StandbyNode and ObserverNode should check if there are new
    edit log entries ready to be consumed. This is the minimum period between
    checking; exponential backoff will be applied if no edits are found and
    dfs.ha.tail-edits.period.backoff-max is configured. By default, no
    backoff is applied.
    Supports multiple time unit suffix (case insensitive), as described
    in dfs.heartbeat.interval.
  </description>
</property>

<property>
  <name>dfs.webhdfs.rest-csrf.enabled</name>
  <value>true</value>
  <description>
    If true, then enables WebHDFS protection against cross-site request forgery
    (CSRF).  The WebHDFS client also uses this property to determine whether or
    not it needs to send the custom CSRF prevention header in its HTTP requests.
  </description>
</property>

<property>
  <name>dfs.balancer.getBlocks.min-block-size</name>
  <value>5242880</value>
  <description>
    Minimum block threshold size in bytes to ignore when fetching a source's
    block list.
  </description>
</property>

</configuration>
