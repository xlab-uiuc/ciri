<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>

<property>
  <name>dfs.datanode.du.reserved.calculator</name>
  <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorAbsolute</value>
  <description>Determines the class of ReservedSpaceCalculator to be used for
    calculating disk space reservedfor non-HDFS data. The default calculator is
    ReservedSpaceCalculatorAbsolute which will use dfs.datanode.du.reserved
    for a static reserved number of bytes. ReservedSpaceCalculatorPercentage
    will use dfs.datanode.du.reserved.pct to calculate the reserved number
    of bytes based on the size of the storage. ReservedSpaceCalculatorConservative and
    ReservedSpaceCalculatorAggressive will use their combination, Conservative will use
    maximum, Aggressive minimum. For more details see ReservedSpaceCalculator.
  </description>
</property>

<property>
  <name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
  <value>true</value>
  <description>
    If there is a datanode/network failure in the write pipeline,
    DFSClient will try to remove the failed datanode from the pipeline
    and then continue writing with the remaining datanodes. As a result,
    the number of datanodes in the pipeline is decreased.  The feature is
    to add new datanodes to the pipeline.

    This is a site-wide property to enable/disable the feature.

    When the cluster size is extremely small, e.g. 3 nodes or less, cluster
    administrators may want to set the policy to NEVER in the default
    configuration file or disable this feature.  Otherwise, users may
    experience an unusually high rate of pipeline failures since it is
    impossible to find new datanodes for replacement.

    See also dfs.client.block.write.replace-datanode-on-failure.policy
  </description>
</property>

<property>
  <name>dfs.namenode.full.block.report.lease.length.ms</name>
  <value>300000</value>
  <description>
    The number of milliseconds that the NameNode will wait before invalidating
    a full block report lease.  This prevents a crashed DataNode from
    permanently using up a full block report lease.
  </description>
</property>

<property>
  <name>dfs.client.mmap.cache.size</name>
  <value>128</value>
  <description>
    When zero-copy reads are used, the DFSClient keeps a cache of recently used
    memory mapped regions.  This parameter controls the maximum number of
    entries that we will keep in that cache.

    The larger this number is, the more file descriptors we will potentially
    use for memory-mapped files.  mmaped files also use virtual address space.
    You may need to increase your ulimit virtual address space limits before
    increasing the client mmap cache size.

    Note that you can still do zero-copy reads when this size is set to 0.
  </description>
</property>

<property>
  <name>dfs.domain.socket.path</name>
  <value>/valid/file1</value>
  <description>
    Optional.  This is a path to a UNIX domain socket that will be used for
    communication between the DataNode and local HDFS clients.
    If the string "_PORT" is present in this path, it will be replaced by the
    TCP port of the DataNode.
  </description>
</property>

<property>
  <name>dfs.client.deadnode.detection.probe.deadnode.interval.ms</name>
  <value>120000</value>
    <description>
      Interval time in milliseconds for probing dead node behavior.
    </description>
</property>

<property>
  <name>dfs.namenode.quota.init-threads</name>
  <value>4</value>
  <description>
    The number of concurrent threads to be used in quota initialization. The
    speed of quota initialization also affects the namenode fail-over latency.
    If the size of name space is big, try increasing this.
  </description>
</property>

<property>
  <name>dfs.webhdfs.rest-csrf.methods-to-ignore</name>
  <value>OPTIONS</value>
  <description>
    A comma-separated list of HTTP methods that do not require HTTP requests to
    include a custom header when protection against cross-site request forgery
    (CSRF) is enabled for WebHDFS by setting dfs.webhdfs.rest-csrf.enabled to
    true.  The WebHDFS client also uses this property to determine whether or
    not it needs to send the custom CSRF prevention header in its HTTP requests.
  </description>
</property>

</configuration>
