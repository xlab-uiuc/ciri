Raw json:

{'hasError': True, 'errParameter': ['dfs.datanode.du.reserved.calculator', 'dfs.client.block.write.replace-datanode-on-failure.enable', 'dfs.namenode.full.block.report.lease.length.ms', 'dfs.client.mmap.cache.size', 'dfs.domain.socket.path', 'dfs.client.deadnode.detection.probe.deadnode.interval.ms', 'dfs.namenode.quota.init-threads', 'dfs.webhdfs.rest-csrf.methods-to-ignore'], 'reason': ["The property 'dfs.datanode.du.reserved.calculator' does not exist in Hadoop 3.3.0. The configuration for reserved space calculation is not customizable in this manner.", "The property 'dfs.client.block.write.replace-datanode-on-failure.enable' is correctly named but its necessity and the description provided might not accurately reflect the default behavior or available options in Hadoop 3.3.0 without further context.", "The property 'dfs.namenode.full.block.report.lease.length.ms' does not exist in Hadoop 3.3.0. The management of full block report leases is not exposed as a configurable property.", "The property 'dfs.client.mmap.cache.size' is not a standard configuration property in Hadoop 3.3.0. Zero-copy reads are managed differently and do not have a cache size configuration.", "The property 'dfs.domain.socket.path' is valid, but the description implies functionality ('_PORT' replacement) that does not exist in Hadoop 3.3.0.", "The property 'dfs.client.deadnode.detection.probe.deadnode.interval.ms' does not exist in Hadoop 3.3.0. Dead node detection is managed through other mechanisms.", "The property 'dfs.namenode.quota.init-threads' does not exist in Hadoop 3.3.0. Quota initialization and management do not have a configurable number of threads in this version.", "The property 'dfs.webhdfs.rest-csrf.methods-to-ignore' is valid, but the specificity of the CSRF configuration for WebHDFS might not be accurately represented in Hadoop 3.3.0 without further context."]}

Final result:

There are 8 misconfiguration parameters in the input: dfs.datanode.du.reserved.calculator	dfs.client.block.write.replace-datanode-on-failure.enable	dfs.namenode.full.block.report.lease.length.ms	dfs.client.mmap.cache.size	dfs.domain.socket.path	dfs.client.deadnode.detection.probe.deadnode.interval.ms	dfs.namenode.quota.init-threads	dfs.webhdfs.rest-csrf.methods-to-ignore
[Ciri] Reason for dfs.datanode.du.reserved.calculator: The property 'dfs.datanode.du.reserved.calculator' does not exist in Hadoop 3.3.0. The configuration for reserved space calculation is not customizable in this manner.
[Ciri] Reason for dfs.client.block.write.replace-datanode-on-failure.enable: The property 'dfs.client.block.write.replace-datanode-on-failure.enable' is correctly named but its necessity and the description provided might not accurately reflect the default behavior or available options in Hadoop 3.3.0 without further context.
[Ciri] Reason for dfs.namenode.full.block.report.lease.length.ms: The property 'dfs.namenode.full.block.report.lease.length.ms' does not exist in Hadoop 3.3.0. The management of full block report leases is not exposed as a configurable property.
[Ciri] Reason for dfs.client.mmap.cache.size: The property 'dfs.client.mmap.cache.size' is not a standard configuration property in Hadoop 3.3.0. Zero-copy reads are managed differently and do not have a cache size configuration.
[Ciri] Reason for dfs.domain.socket.path: The property 'dfs.domain.socket.path' is valid, but the description implies functionality ('_PORT' replacement) that does not exist in Hadoop 3.3.0.
[Ciri] Reason for dfs.client.deadnode.detection.probe.deadnode.interval.ms: The property 'dfs.client.deadnode.detection.probe.deadnode.interval.ms' does not exist in Hadoop 3.3.0. Dead node detection is managed through other mechanisms.
[Ciri] Reason for dfs.namenode.quota.init-threads: The property 'dfs.namenode.quota.init-threads' does not exist in Hadoop 3.3.0. Quota initialization and management do not have a configurable number of threads in this version.
[Ciri] Reason for dfs.webhdfs.rest-csrf.methods-to-ignore: The property 'dfs.webhdfs.rest-csrf.methods-to-ignore' is valid, but the specificity of the CSRF configuration for WebHDFS might not be accurately represented in Hadoop 3.3.0 without further context.
