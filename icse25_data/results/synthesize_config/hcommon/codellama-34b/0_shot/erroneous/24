Raw json:

{'hasError': True, 'errParameter': ['fs.ftp.host.port', 'fs.s3a.fast.upload.buffer', 'fs.s3a.metadatastore.authoritative', 'fs.s3a.select.input.csv.header', 'ipc.[port_number].faircallqueue.multiplexer.weights', 'hadoop.caller.context.max.size'], 'reason': ['The property fs.ftp.host.port is not valid for the version of Hadoop you are using.', 'The property fs.s3a.fast.upload.buffer is not valid for the version of Hadoop you are using.', 'The property fs.s3a.metadatastore.authoritative is not valid for the version of Hadoop you are using.', 'The property fs.s3a.select.input.csv.header is not valid for the version of Hadoop you are using.', 'The property ipc.[port_number].faircallqueue.multiplexer.weights is not valid for the version of Hadoop you are using.', 'The property hadoop.caller.context.max.size is not valid for the version of Hadoop you are using.']}

Final result:

There are 6 misconfiguration parameters in the input: fs.ftp.host.port	fs.s3a.fast.upload.buffer	fs.s3a.metadatastore.authoritative	fs.s3a.select.input.csv.header	ipc.[port_number].faircallqueue.multiplexer.weights	hadoop.caller.context.max.size
[Ciri] Reason for fs.ftp.host.port: The property fs.ftp.host.port is not valid for the version of Hadoop you are using.
[Ciri] Reason for fs.s3a.fast.upload.buffer: The property fs.s3a.fast.upload.buffer is not valid for the version of Hadoop you are using.
[Ciri] Reason for fs.s3a.metadatastore.authoritative: The property fs.s3a.metadatastore.authoritative is not valid for the version of Hadoop you are using.
[Ciri] Reason for fs.s3a.select.input.csv.header: The property fs.s3a.select.input.csv.header is not valid for the version of Hadoop you are using.
[Ciri] Reason for ipc.[port_number].faircallqueue.multiplexer.weights: The property ipc.[port_number].faircallqueue.multiplexer.weights is not valid for the version of Hadoop you are using.
[Ciri] Reason for hadoop.caller.context.max.size: The property hadoop.caller.context.max.size is not valid for the version of Hadoop you are using.
